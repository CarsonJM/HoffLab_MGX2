#------------------------------------
# Sample input information
#------------------------------------
sample_info:
  # path to sample information sheet in TSV format (absolute path)
  sample_list_path: "config/samples.tsv"
  # name of the study (for metadata)
  study_name: 'HoffLab_MGX_test'
  # information about the ecosystem the sample was obtained from (for metadata)
  sample_ecosystem: "in_silico"


#------------------------------------
# Resources path
#------------------------------------
# path to resources folder (so large databases can be re-used)
resources_path: "/labdata3/hoffdata/Shared/resources"


#------------------------------------
# Rule customization
#------------------------------------
### Read Preprocessing ###
# 1. Deduplicate reads using clumpify (OPTIONAL)
clumpify:
  # run clumpify?
  run_clumpify: True
  # extra clumpify arguments? (input as string)
  # Note: the recommended arguments are: "dedupe=t optical=t spany=t adjacent=t"
  extra_args: "dedupe=t optical=t spany=t adjacent=t"

# 2. Quality filter and remove human reads with KneadData (REQUIRED)
kneaddata:
  # extra kneaddata arguments? (input as a string)
  # Note: the recommended arguments are: "--remove-intermediate-output --verbose --trimmomatic <trimmomatic path>"
  # Note: do not add --log option, as it will change the output which is used for determining read counts downstream
  extra_args: "--remove-intermediate-output --verbose --trimmomatic $CONDA_PREFIX/share/trimmomatic-0.39-2"


### Read-Based Taxonomy ###
# 1. Taxonomically annotate reads using MetaPhlan (REQUIRED)
metaphlan:
  # extra metaphlan arguments? (input as string)
  extra_args: "-t rel_ab_w_read_stats --input_type fastq"
